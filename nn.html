<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>The miniaturist's neural network (WIP)</title>
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="assets/style.css"/>
<link rel="icon" href="assets/favicon.ico" type="image/x-icon">
<style>
mjx-container[jax="CHTML"] {
overflow-x: auto !important;
}
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">The miniaturist's neural network (WIP)</h1>
<div id="outline-container-org662c756" class="outline-2">
<h2 id="org662c756">Preface</h2>
<div class="outline-text-2" id="text-org662c756">
<p>
We will implement a fully-connected feed-forward neural network<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>, in other words, a
</p>

<details>
<summary>Multilayer perceptron</summary>

<p>
Essentially an optimization problem of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\)
that has exceptionally good properties for <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">approximating</a> other continuous functions on compact subsets of \(\mathbb{R}^n\).
A multilayer perceptron (MLP) of \(L\) layers, features \(x_i\), and targets \(y_i\) has the following recursive definition:
</p>

\begin{equation*}
  f = \begin{cases}
    a_i^{(0)} = x_i & \\
    a_i^{(l)} = \sigma\left( \sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}\, a_j^{(l-1)} + b_i^{(l)} \right) = \sigma\left( z_i^{(l)} \right) & l \in [1, L]
  \end{cases}
\end{equation*}

<p>
where \(a_i^{(l)}\) is the activation of the layer \(l\), \(w_{ij}^{(l)}\) is the weight connecting the \(j\)-th
neuron in layer \(l-1\) to the \(i\)-th neuron in layer \(l\), \(b_i^{(l)}\)‚Äã is the bias for the \(i\)-th
neuron in layer \(l\), \(N_l\) is the number of neurons in layer \(l\), and \(\sigma\) is the activation function
(the <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a> in our case).
</p>

</details>

<p>
As a reference implementation, we will use <a href="https://github.com/glouw/tinn">Tinn</a>, which is a MLP of a single hidden layer, written in pure C with
no dependencies<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>. As usual, we will set the stage by importing and defining some utility functions,
namely plotting, random number generation, and matrix product: 
</p>

<div class="org-src-container">
<pre class="src src-bqn">Setplot‚ÄøPlot ‚Üê ‚Ä¢Import "../bqn-utils/plots.bqn"
U ‚Üê 0.5-Àú‚Ä¢rand.Range‚üú0
M ‚Üê +Àù‚àò√ó‚éâ1‚Äø‚àû
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd37860c" class="outline-2">
<h2 id="orgd37860c">Tinn's diminution</h2>
<div class="outline-text-2" id="text-orgd37860c">
<p>
The original C implementation has 175 lines excluding the optimization loop. The BQN version has only
</p>

<div class="org-src-container">
<pre class="src src-bqn">Minn ‚Üê {r‚Äølùïäùï©:
  C ‚áê +`2√∑Àú¬∑√óÀú-
  A‚ÄøDA ‚Üê ‚ü®1√∑1+‚ãÜ‚àò-, ‚ä¢√ó1‚ä∏-‚ü©
  F ‚áê {ùï®ùïäb‚Äøw: A¬®b+w‚ä∏Mùï®}`
  B ‚Üê {
    do ‚Üê (DA‚àò‚ä£√óùïó-‚ä¢)¬¥‚ä¢¬¥¬®ùï®‚Äøùï©
  }
  l(U‚öá1-‚üú1‚àò‚â†‚ãà¬∑&lt;Àò2‚ä∏‚Üï)‚ä∏{fs‚Äøtsùïäb‚Äøw:
    B fs F b‚ãà¬®w
  }¬¥ùï©
}
</pre>
</div>

<p>
Training the MLP involves a two-stage process for each input: forward propagation followed by backpropagation,
during which the neural network's weight matrices are adjusted to minimize a cost function. The second step
is often shrouded in mystery, despite being nothing more than
</p>

<details>
<summary>An application of the chain rule</summary>

<p>
Before introducing a vectorized representation of the backpropagation algorithm, it is important to note that we use a
quadratic loss function \( C = \frac{1}{2} \| a^{(L)} - y \|^2 \), and optimize the network using <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>.
Using the MLP definition in the first collapsible and the chain rule, we can compute the error at the output
layer \(L\) with the following Hadamard product:
</p>

\begin{equation*}
  \delta^{(L)} = \left( a^{(L)} - y \right) \odot \sigma'\left( z^{(L)} \right)
\end{equation*}

<p>
The sigmoid is the solution to the logistic differential equation, can you work out what its derivative is? Then,
the total derivative and the chain rule come to rescue once again to express the error of the hidden layers \(l\in [1,L)\):
</p>

\begin{equation*}
  \delta^{(l)} = \left({W^{(l+1)}}^\top \delta^{(l+1)}\right) \odot \sigma'\left( z^{(l)} \right)
\end{equation*}

<p>
where we have introduced the matrix form of the weights \(W^{(l)}\). The gradient of the cost function is:
</p>

\begin{equation*}
  \nabla C = \left\{ \frac{\partial C}{\partial W^{(l)}} = \delta^{(l)} {a^{(l-1)}}^\top, \quad \frac{\partial C}{\partial b^{(l)}} = \delta^{(l)} \right\}_{l=1}^{L}
\end{equation*}

<p>
Finally, we can do a gradient descent step with a learning rate \(\eta\), which can be possibly annealed:
</p>

\begin{equation*}
  \Delta\left\{W^{(l)}, b^{(l)}\right\}_{l=1}^{L} \gets -\eta\nabla C
\end{equation*}

<p>
For a straightforward derivation, refer to the dedicated section in Nielsen's <a href="http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional)">book</a>. For a rigorous
presentation, see <a href="https://arxiv.org/abs/2107.09384">arXiv:2107.09384</a>.
</p>

</details>
</div>
</div>
<div id="outline-container-orga896b10" class="outline-2">
<h2 id="orga896b10">Learning the logistic map</h2>
<div class="outline-text-2" id="text-orga896b10">
<p>
<code>_minn</code> should handle digit recognition just fine<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>. However, I would like to switch clich√©s for the demonstration.
Instead, we will use it to learn the logistic map<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>. This is a quintessential example of how chaos can emerge from simple systems.
Moreover, it is not so trivial to approximate: the recurrence lacks a <a href="https://mathworld.wolfram.com/LogisticMap.html">closed-form</a> solution, and has been a subject of study in
the context of neural networks<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>.
</p>

<div class="org-src-container">
<pre class="src src-bqn">L ‚Üê ‚ä£√ó1‚ä∏-√ó‚ä¢
np‚Äøns‚Äøri‚Äørf‚Äødr ‚Üê 600‚Äø50‚Äø2.8‚Äø4‚Äø0.01

r ‚Üê ‚Üï‚àò‚åà‚åæ((ri+dr√ó‚ä¢)‚Åº)rf
@ ‚ä£ m ‚Üê r L‚çü((np-ns)+‚Üïns)¬® 0 ‚Ä¢rand.RangeÀú ‚â†r
</pre>
</div>

<p>
Let‚Äôs see if we‚Äôve gotten the numbers right after learning. But then again, what is a number that a man may know it<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>&#x2026;
</p>

<div class="org-src-container">
<pre class="src src-bqn">)r Setplot "scatter" ‚ãÑ ‚Ä¢Out¬® Plot¬¥  m {ns‚Üê‚â†‚äëùï® ‚ãÑ (&gt;ùï®)‚ãàÀú‚àò‚Äøns‚•äns/ùï©} r
</pre>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
This post is not intended to be an introduction to the topic. There are excellent
<a href="https://www.3blue1brown.com/topics/neural-networks">videos</a>, <a href="https://compphysics.github.io/MachineLearning/doc/web/course.html">lecture notes</a>, <a href="https://arxiv.org/pdf/2105.04026">papers</a>, and <a href="https://deeplearningtheory.com/">books</a> that do this better than I could. I will provide only
the essential context to ensure the reading is self-contained.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Programming by poking is the antithesis of this blog's ethos.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
You can try using UCI's <a href="https://archive.ics.uci.edu/dataset/178/semeion+handwritten+digit">Semeion Handwritten Digit</a> dataset, like Tinn does.
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Isn't it fascinating how closely related and yet so different the logistic map and the logistic function are?
The former can be thought of as a discrete version of \(\dot{f} = f(1 - f)\), but whereas this ODE has a boring
sigmoid solution, the logistic map yields beautiful bifurcation diagrams.
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
See, for instance, <a href="https://arxiv.org/abs/2409.07468">arXiv:2409.07468</a>.
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
&#x2026; and a man that he may know a number? Thus <a href="https://www.nsl.com/k/parry/mcculloch_what-is-a-number.pdf">spoke</a> Warren McCulloch, a profoundly inspiring figure.
</p>

<div style="text-align: center; font-size: 2em; padding: 20px 0;">
  <a href="https://panadestein.github.io/blog/" style="text-decoration: none;">‚äë‚àò‚àû</a>
</div></div></div>


</div>
</div></div>
</body>
</html>
