# -*- eval: (face-remap-add-relative 'default '(:family "BQN386 Unicode" :height 180)); -*-
#+TITLE: The miniaturist's neural network (WIP)
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="assets/style.css"/>
#+HTML_HEAD: <link rel="icon" href="assets/favicon.ico" type="image/x-icon">
#+HTML_HEAD: <style>
#+HTML_HEAD: mjx-container[jax="CHTML"] {
#+HTML_HEAD:   overflow-x: auto !important;
#+HTML_HEAD: }
#+HTML_HEAD: </style>

** Preface

We will implement a fully-connected feed-forward neural network[fn:1], in other words, a

#+begin_export html
<details>
<summary>Multilayer perceptron</summary>
#+end_export

Essentially an optimization problem of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\)
that has exceptionally good properties for [[https://en.wikipedia.org/wiki/Universal_approximation_theorem][approximating]] other continuous functions on compact subsets of \(\mathbb{R}^n\).
A multilayer perceptron (MLP) of \(L\) layers, features \(x_i\), and targets \(y_i\) has the following recursive definition:

\begin{equation*}
  f = \begin{cases}
    a_i^{(0)} = x_i & \\
    a_i^{(l)} = \sigma\left( \sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}\, a_j^{(l-1)} + b_i^{(l)} \right) = \sigma\left( z_i^{(l)} \right) & l \in [1, L]
  \end{cases}
\end{equation*}

where \(a_i^{(l)}\) is the activation of the layer \(l\), \(w_{ij}^{(l)}\) is the weight connecting the \(j\)-th
neuron in layer \(l-1\) to the \(i\)-th neuron in layer \(l\), \(b_i^{(l)}\)​ is the bias for the \(i\)-th
neuron in layer \(l\), \(N_l\) is the number of neurons in layer \(l\), and \(\sigma\) is the activation function
(the [[https://en.wikipedia.org/wiki/Logistic_function][logistic function]] in our case).

#+begin_export html
</details>
#+end_export

As a reference implementation, we will use [[https://github.com/glouw/tinn][Tinn]], which is a MLP of a single hidden layer, written in pure C with
no dependencies[fn:2]. As usual, we will set the stage by importing and defining some utility functions,
namely plotting, random number generation, and matrix product: 

#+begin_src bqn :tangle ./bqn/nn.bqn
  Setplot‿Plot ← •Import "../bqn-utils/plots.bqn"
  U ← 0.5-˜•rand.Range⟜0
  M ← +˝∘×⎉1‿∞
#+end_src

#+RESULTS:
: +˝∘×⎉⟨ 1 ∞ ⟩

** Tinn's diminution

The original C implementation has 175 lines excluding the optimization loop. The BQN version has only 10.

#+begin_src bqn :tangle ./bqn/nn.bqn
  Minn ← {r‿l𝕊𝕩:
    A‿DA ← ⟨1÷1+⋆∘-, ⊢×1⊸-⟩
    BP ← {ts‿w𝕊𝕩:
      do ← (-⟜ts×DA)⊢´𝕩
      𝕩×⌜¨do∾(⊑do){d𝕊w‿ly: (DA ly)×d M w}`⋈¨´(1↓⌽)¨w‿𝕩
    }
    FP ← {𝕨𝕊b‿w: A¨b+𝕨M w}`⟜(⋈¨´)
    nn ← 𝕩{fs‿ts𝕊b‿w: b⋈w-r× ts‿w BP fs FP𝕩}´˜(U⚇1-⟜1∘≠⋈·<˘2⊸↕)l
    E ⇐ ⊢´FP⟜nn  
  }
#+end_src

#+RESULTS:
: (function block)

Training the MLP involves a two-stage process for each input: forward propagation followed by backpropagation,
during which the neural network's weight matrices are adjusted to minimize a cost function. The second step
is often shrouded in mystery, despite being nothing more than

#+begin_export html
<details>
<summary>An application of the chain rule</summary>
#+end_export

Before introducing a vectorized representation of the backpropagation algorithm, it is important to note that we use a
quadratic loss function \( C = \frac{1}{2} \| a^{(L)} - y \|^2 \), and optimize the network using [[https://en.wikipedia.org/wiki/Gradient_descent][gradient descent]].
Using the MLP definition in the first collapsible and the chain rule, we can compute the error at the output
layer \(L\) with the following Hadamard product:

\begin{equation*}
  \delta^{(L)} = \left( a^{(L)} - y \right) \odot \sigma'\left( z^{(L)} \right)
\end{equation*}

The sigmoid is the solution to the logistic differential equation, can you work out what its derivative is? Then,
the total derivative and the chain rule come to rescue once again to express the error of the hidden layers \(l\in [1,L)\):

\begin{equation*}
  \delta^{(l)} = \left({W^{(l+1)}}^\top \delta^{(l+1)}\right) \odot \sigma'\left( z^{(l)} \right)
\end{equation*}

where we have introduced the matrix form of the weights \(W^{(l)}\). The gradient of the cost function is:

\begin{equation*}
  \nabla C = \left\{ \frac{\partial C}{\partial W^{(l)}} = \delta^{(l)} {a^{(l-1)}}^\top, \quad \frac{\partial C}{\partial b^{(l)}} = \delta^{(l)} \right\}_{l=1}^{L}
\end{equation*}

Finally, we can do a gradient descent step with a learning rate \(\eta\), which can be possibly annealed:

\begin{equation*}
  \Delta\left\{W^{(l)}, b^{(l)}\right\}_{l=1}^{L} \gets -\eta\nabla C
\end{equation*}

For a straightforward derivation, refer to the dedicated section in Nielsen's [[http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional)][book]]. For a rigorous
presentation, see [[https://arxiv.org/abs/2107.09384][arXiv:2107.09384]].

#+begin_export html
</details>
#+end_export

** Learning the logistic map

=Minn= should handle digit recognition just fine[fn:3]. However, I would like to switch clichés for the demonstration.
Instead, we will use it to learn the logistic map[fn:4]. This is a quintessential example of how chaos can emerge from simple systems.
Moreover, it is not so trivial to approximate: the recurrence lacks a [[https://mathworld.wolfram.com/LogisticMap.html][closed-form]] solution, and has been a subject of study in
the context of neural networks[fn:5]. First let's generate some reference data:

#+begin_src bqn :tangle ./bqn/nn.bqn
  n‿ri‿rf‿dr ← 1000‿2.8‿4‿0.001
  @ ⊣ td ← {𝕩(⊣∾(⊣×1⊸-×⊢)⍟n)¨0 •rand.Range˜≠𝕩} ↕∘⌈⌾((ri+dr×⊢)⁼)rf
#+end_src

#+RESULTS:
: @

Then we can train =Minn=, and generate some approximated data with the perceptron:

#+begin_src bqn :tangle ./bqn/nn.bqn
  lm ← 0.01‿⟨1, 10, 1⟩ Minn td
  ≠lm.E 2.9
#+end_src

#+RESULTS:
: Error: Mapping: Expected equal shape prefix (1‿10 ≡ ≢𝕨, ⟨10⟩ ≡ ≢𝕩)
: at   nn ← 𝕩{fs‿ts𝕊b‿w: b⋈w-r× ts‿w BP fs FP𝕩}´˜(U⚇1-⟜1∘≠⋈·<˘2⊸↕)l
:                           ^
: at   nn ← 𝕩{fs‿ts𝕊b‿w: b⋈w-r× ts‿w BP fs FP𝕩}´˜(U⚇1-⟜1∘≠⋈·<˘2⊸↕)l
:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
: at lm ← 0.01‿⟨1, 10, 1⟩ Minn td
:                         ^^^^

Let’s see if we’ve gotten the numbers right after learning. But then again, what is a number that a man may know it[fn:6]...

#+NAME: attr_wrap
#+BEGIN_SRC sh :var data="" :results output :exports none :tangle no
  echo "<br/>"
  echo '<div style="display: flex; justify-content: center; width: 100%;">'
  echo '<div style="width: 40%;">'
  echo "$data"
  echo "</div>"
  echo "</div>"
#+END_SRC

#+begin_src bqn :results html :exports both :tangle ./bqn/nn.bqn :post attr_wrap(data=*this*)
  )r Setplot "scatter" ⋄ •Out¨ Plot´  m {ns←≠⊑𝕨 ⋄ (>𝕨)⋈˜∘‿ns⥊ns/𝕩} r
#+end_src

[fn:1] This post is not intended to be an introduction to the topic. There are excellent
[[https://www.3blue1brown.com/topics/neural-networks][videos]], [[https://compphysics.github.io/MachineLearning/doc/web/course.html][lecture notes]], [[https://arxiv.org/pdf/2105.04026][papers]], and [[https://deeplearningtheory.com/][books]] that do this better than I could. I will provide only
the essential context to ensure the reading is self-contained.
[fn:2] Programming by poking is the antithesis of this blog's ethos.
[fn:3] You can try using UCI's [[https://archive.ics.uci.edu/dataset/178/semeion+handwritten+digit][Semeion Handwritten Digit]] dataset, like Tinn does.
[fn:4]  Isn't it fascinating how closely related and yet so different the logistic map and the logistic function are?
The former can be thought of as a discrete version of \(\dot{f} = f(1 - f)\), but whereas this ODE has a boring
sigmoid solution, the logistic map yields beautiful bifurcation diagrams.
[fn:5] See, for instance, [[https://arxiv.org/abs/2409.07468][arXiv:2409.07468]].
[fn:6] ... and a man that he may know a number? Thus [[https://www.nsl.com/k/parry/mcculloch_what-is-a-number.pdf][spoke]] Warren McCulloch, a profoundly inspiring figure.

#+BEGIN_EXPORT html
  <div style="text-align: center; font-size: 2em; padding: 20px 0;">
    <a href="https://panadestein.github.io/blog/" style="text-decoration: none;">⊑∘∞</a>
  </div>
#+END_EXPORT
